using PyTensorFlow
using PyCall
using LinearAlgebra
using PyPlot
if !(@isdefined initialized)
    py"""
    import tensorflow as tf
    from tensorflow.python.framework import ops
    lib${OperatorName} = tf.load_op_library('build/lib${OperatorName}.${dylibso}')
    @ops.RegisterGradient("${OperatorName}")
    def _gradcc(op, grad):
${GetAttr}
        return lib${OperatorName}.${operator_name}_grad(grad, *op.inputs, **attr_dict)
    """
    global initialized = true
end

${operator_name} = py"lib${OperatorName}.${operator_name}"

# TODO: 
sess = Session()
init(sess)
run(sess, u)

# TODO: 


# gradient check -- v
function scalar_function(m)
    return sum(tanh(${operator_name}(m)))
end

m_ = constant(rand(10,20))
v_ = rand(10,20)
y_ = scalar_function(m_)
dy_ = gradients(y_, m_)
ms_ = Array{Any}(undef, 5)
ys_ = Array{Any}(undef, 5)
s_ = Array{Any}(undef, 5)
w_ = Array{Any}(undef, 5)
gs_ =  @. 1 / 10^(1:5)

for i = 1:5
    g_ = gs_[i]
    ms_[i] = m_ + g_*v_
    ys_[i] = scalar_function(ms_[i])
    s_[i] = ys_[i] - y_
    w_[i] = s_[i] - g_*sum(v_.*dy_)
end

sess = Session()
init(sess)
sval_ = run(sess, s_)
wval_ = run(sess, w_)
close("all")
loglog(gs_, abs.(sval_), "*-", label="finite difference")
loglog(gs_, abs.(wval_), "+-", label="automatic differentiation")
loglog(gs_, gs_.^2 * 0.5*abs(wval_[1])/gs_[1]^2, "--",label="\$$\\mathcal{O}(\\gamma^2)\$$")
loglog(gs_, gs_ * 0.5*abs(sval_[1])/gs_[1], "--",label="\$$\\mathcal{O}(\\gamma)\$$")

plt[:gca]()[:invert_xaxis]()
legend()
xlabel("\$$\\gamma\$$")
ylabel("Error")
